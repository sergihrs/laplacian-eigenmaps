\documentclass[11pt]{beamer}

% Configuración de idioma y codificación
\usepackage[utf8]{inputenc}
% \usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
 
% Selección de tema (Madrid es clásico y tiene pies de página informativos)
\usetheme{Madrid}
\usecolortheme{default} % Puedes cambiar 'beaver' por 'default', 'dolphin', etc.
 
% Metadatos del documento
\title[Proyecto 7: Laplacian Eigenmaps]{Laplacian Eigenmaps for Dimensionality Reduction}
\subtitle{Differential Methods for AI}
\author[Máster IA]{Sergio Herreros Pérez, Mario Kroll Merino, Carlos Mazuecos Reíllo, José Andrés Ridruejo Tuñón}
\date{} % Dejar vacío o poner \today
 
% Configuración para numeración de páginas (ej. 1/8)
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number]
 
\begin{document}

% PEPE
\begin{frame}
    \titlepage
\end{frame}

\begin{frame}{Laplacian Eigenmaps: The Core Intuition}
    \begin{block}{Goal}
        Map high-dimensional data $x_1, \dots, x_N \in \mathbb{R}^D$ to low-dimensional points $\mathbf{y}_1, \dots, \mathbf{y}_N \in \mathbb{R}^m$ (where $m \ll D$) while preserving \textbf{local neighborhood information}.
    \end{block}

    \vspace{0.5cm}
    \textbf{The Fundamental Idea:}
    \begin{itemize}
        \item If two points $x_i$ and $x_j$ are close in the original space (high similarity), they should be mapped close together in the new space.
        \item We treat this as an optimization problem minimizing the stretching of edges in the neighborhood graph.
    \end{itemize}
\end{frame}

\begin{frame}{Step 1: Graph Construction \& Weights}
    First, we define the geometry of the data manifold.

    \begin{enumerate}
        \item \textbf{Adjacency (Neighbors):} Use $k$-Nearest Neighbors (k-NN). To ensure the graph is undirected, we enforce symmetry:
              \[
                  \text{Connect } i \text{ and } j \text{ if } i \in kNN(j) \textbf{ OR } j \in kNN(i).
              \]
        \item \textbf{Weights ($W$):} Assign weights $W_{ij}$ (Heat Kernel):
              \[
                  W_{ij} = e^{-\frac{||x_i - x_j||^2}{t}} \quad \text{if connected, else } 0.
              \]
        \item \textbf{Degree Matrix ($D$):} Diagonal matrix measuring density.
              \[
                  D_{ii} = \sum_{j} W_{ij}
              \]
    \end{enumerate}
\end{frame}

\begin{frame}{Step 2: The Optimization Problem (1D Case)}
    Let's try to map the data to a \textbf{single line} ($m=1$).
    Let $\mathbf{y} = (y_1, y_2, \dots, y_N)^T$ be the coordinate vector.

    We want to minimize the weighted distance between neighbors:

    \begin{block}{Objective Function}
        \[
            \min_{\mathbf{y}} \sum_{i,j} (y_i - y_j)^2 W_{ij}
        \]
    \end{block}

    \begin{itemize}
        \item If $W_{ij}$ is large (points are close), $y_i$ and $y_j$ \textbf{must} be close to minimize the cost.
        \item If $W_{ij} = 0$, the distance $(y_i - y_j)$ doesn't matter.
    \end{itemize}
\end{frame}

\begin{frame}{Step 3: From Summation to Matrix Form}
    We can rewrite the sum using algebraic manipulation:

    \begin{align*}
        \sum_{i,j} (y_i - y_j)^2 W_{ij} & = \sum_{i,j} (y_i^2 + y_j^2 - 2y_i y_j) W_{ij}                               \\
                                        & = \sum_{i} y_i^2 D_{ii} + \sum_{j} y_j^2 D_{jj} - 2\sum_{i,j} y_i y_j W_{ij} \\
                                        & = 2\mathbf{y}^T D \mathbf{y} - 2\mathbf{y}^T W \mathbf{y}                    \\
                                        & = 2\mathbf{y}^T (D - W) \mathbf{y}
    \end{align*}

    \begin{alertblock}{The Graph Laplacian}
        Since $L = D - W$, the objective function becomes:
        \[
            \text{Cost}(\mathbf{y}) = 2 \mathbf{y}^T L \mathbf{y}
        \]
    \end{alertblock}
\end{frame}

\begin{frame}{Step 4: Constraints and Lagrange Multipliers}
    To prevent all points from collapsing to a single point, we impose a constraint to fix the scale and handle density:
    \[
        \mathbf{y}^T D \mathbf{y} = 1
    \]

    Now we solve using \textbf{Lagrange Multipliers}:
    \[
        \mathcal{L}(\mathbf{y}, \lambda) = \mathbf{y}^T L \mathbf{y} - \lambda (\mathbf{y}^T D \mathbf{y} - 1)
    \]
    Taking the derivative with respect to $\mathbf{y}$ and setting to 0:
    \[
        2 L \mathbf{y} - 2 \lambda D \mathbf{y} = 0 \implies \boxed{L \mathbf{y} = \lambda D \mathbf{y}}
    \]

    This is the \textbf{Generalized Eigenvalue Problem}.
\end{frame}

\begin{frame}{Step 5: Why Smallest Eigenvalues?}
    We have found that the optimal $\mathbf{y}$ must be an eigenvector. But which one?
    Let's check the cost of a specific eigenvector solution $\mathbf{y}$:

    \[
        \text{Cost} = \mathbf{y}^T L \mathbf{y}
    \]
    Since $L \mathbf{y} = \lambda D \mathbf{y}$, we substitute:
    \[
        \mathbf{y}^T L \mathbf{y} = \mathbf{y}^T (\lambda D \mathbf{y}) = \lambda (\mathbf{y}^T D \mathbf{y})
    \]
    Using our constraint $\mathbf{y}^T D \mathbf{y} = 1$:
    \[
        \text{Cost} = \lambda
    \]

    \begin{block}{Conclusion}
        To \textbf{minimize the cost}, we must choose the eigenvectors with the \textbf{smallest eigenvalues} $\lambda$.
    \end{block}
\end{frame}

\begin{frame}{Step 6: Generalizing to $m$ Dimensions}
    \begin{itemize}
        \item $\lambda_0 = 0$: Corresponds to eigenvector $\mathbf{1}$ (constant vector). Maps all points to a single spot. \textbf{Discard it.}
        \item $\lambda_1, \dots, \lambda_m$: The smallest non-zero eigenvalues.
    \end{itemize}

    \vspace{0.3cm}
    \textbf{The Embedding Map:}
    For embedding into $m$ dimensions, we use the eigenvectors $\mathbf{f}_1, \dots, \mathbf{f}_m$ associated with $0 < \lambda_1 \le \dots \le \lambda_m$.

    \[
        x_i \mapsto \left( \mathbf{f}_1(i), \mathbf{f}_2(i), \dots, \mathbf{f}_m(i) \right)
    \]

    \begin{block}{Summary}
        The coordinates in the low-dimensional space are literally the values of the Laplacian eigenvectors.
    \end{block}
\end{frame}

% MAZUECOS
\begin{frame}{Laplace–Beltrami: From Continuous to Discrete}

    \vspace{0.3cm}

    \begin{columns}[t]
        % --- LEFT COLUMN: CONTINUOUS ---
        \begin{column}{0.48\textwidth}
            \centering
            \textbf{\textcolor{blue}{Continuous Manifold}}

            \vspace{0.2cm}
            \emph{Minimize Gradient Energy}
            \[
                \min_{\|u\|=1} \int_\Omega \|\nabla u\|^2 \, dx
            \]
            \vspace{0.1cm}
            $\downarrow$ \small{leads to}
            \vspace{0.1cm}

            \textbf{Laplace-Beltrami Operator}
            \[
                -\Delta u = \lambda u
            \]
        \end{column}

        % --- VERTICAL SEPARATOR (Optional visual aid) ---
        \vrule{}

        % --- RIGHT COLUMN: DISCRETE ---
        \begin{column}{0.48\textwidth}
            \centering
            \textbf{\textcolor{blue}{Discrete Graph}}

            \vspace{0.2cm}
            \emph{Minimize Edge Tension}
            \[
                \min_{\mathbf{f}^T D \mathbf{f}=1} \sum_{i,j} W_{ij}(f_i - f_j)^2
            \]
            \vspace{0.1cm}
            $\downarrow$ \small{leads to}
            \vspace{0.1cm}

            \textbf{Graph Laplacian}
            \[
                L \mathbf{f} = \lambda D \mathbf{f}
            \]
        \end{column}
    \end{columns}

    \vspace{0.5cm}

    \begin{block}{The Correspondence}
        \centering
        \begin{tabular}{r c l}
            Local Variation $(\nabla u)^2$ & $\longleftrightarrow$ & Edge Difference $(f_i - f_j)^2$ \\
            Laplace Operator $-\Delta$     & $\longleftrightarrow$ & Graph Matrix $L = D-W$          \\
            Manifold Harmonics             & $\longleftrightarrow$ & Graph Eigenvectors              \\
        \end{tabular}
    \end{block}
\end{frame}

\begin{frame}{Laplacian Eigenmaps in Graph-Based Learning}
    \begin{itemize}
        \item Laplacian Eigenmaps provide the spectral foundation for many graph learning algorithms:
              \begin{itemize}
                  \item \textbf{Spectral Clustering}: uses the smallest non-zero Laplacian eigenvectors to partition graphs.
                  \item \textbf{Graph Convolutional Networks (GCNs)}: graph convolutions are defined using the Laplacian spectrum.
              \end{itemize}

              \vspace{0.3cm}

        \item The Laplacian encodes the local geometry of the dataset by capturing similarity relations through weights $w_{ij}$.
        \item Eigenvectors provide smooth representations that respect this geometry.
    \end{itemize}
\end{frame}

\begin{frame}{Connection to the Finite Element Method (FEM)}
    \begin{block}{Shared Variational Principle}
        Both FEM and Laplacian Eigenmaps are based on minimizing an energy associated with the Laplacian:
        \[
            \int_\Omega |\nabla u|^2 \quad \longleftrightarrow \quad
            \sum_{i,j} w_{ij}(f_i - f_j)^2 = f^\top L f.
        \]
    \end{block}

    \vspace{0.3cm}

    \begin{itemize}
        \item FEM approximates $\Delta$ on a mesh; Laplacian Eigenmaps approximate $\Delta$ on a \textbf{similarity graph}.
        \item In both cases, low-energy eigenfunctions correspond to the smoothest modes.
        \item This provides a rigorous bridge between PDE-based models and graph learning.
    \end{itemize}
\end{frame}

\begin{frame}{From Continuous Geometry to Discrete Data}
    \begin{itemize}
        \item Laplacian Eigenmaps allow transferring ideas from differential geometry to data analysis:
              \begin{itemize}
                  \item \textbf{Diffusion}: heat flow on a manifold $\leftrightarrow$ diffusion processes on graphs.
                  \item \textbf{Smoothness}: low-oscillation eigenfunctions $\leftrightarrow$ low-variation graph signals.
                  \item \textbf{Eigenmodes}: Laplace–Beltrami eigenfunctions $\leftrightarrow$ graph Laplacian eigenvectors.
              \end{itemize}

              \vspace{0.3cm}

        \item This creates a unified framework connecting:
              \begin{itemize}
                  \item PDEs and variational principles,
                  \item manifold geometry,
                  \item and machine learning on graphs.
              \end{itemize}
              \vspace{0.3cm}

        \item The result: geometric structure of data becomes accessible even in discrete, high-dimensional settings.
    \end{itemize}
\end{frame}

% SERGI
\begin{frame}{Swiss Roll Dataset}
    \begin{figure}
        \centering
        \includegraphics[width=0.95\textwidth]{./images/swiss_roll_3d.png}
        \caption{3D Visualization of the Swiss Roll Dataset}
    \end{figure}
\end{frame}

\begin{frame}{Laplacian Eigenmaps Embedding of Swiss Roll}
    \begin{figure}
        \centering
        \includegraphics[width=0.7\textwidth]{./images/le_swiss_roll_k15_s2.0.png}
        \caption{2D Embedding of the Swiss Roll Dataset using Laplacian Eigenmaps}
    \end{figure}
\end{frame}

\begin{frame}{Effect of Neighbors (k) on Embedding}
    \begin{figure}
        \centering
        \includegraphics[width=0.66\textwidth]{./images/le_swiss_roll_neighbours.png}
        \caption{Effect of Varying Number of Neighbors (k) on the Embedding}
    \end{figure}
\end{frame}

\begin{frame}{Effect of Sigma (Kernel Bandwidth) on Embedding}
    \begin{figure}
        \centering
        \includegraphics[width=0.66\textwidth]{./images/le_swiss_roll_sigmas.png}
        \caption{Effect of Varying Sigma (Kernel Bandwidth) on the Embedding}
    \end{figure}
\end{frame}

\begin{frame}{PCA vs Laplacian Eigenmaps on Swiss Roll}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{./images/pca_vs_le.png}
        \caption{Comparison of PCA and Laplacian Eigenmaps on the Swiss Roll Dataset}
    \end{figure}
\end{frame}

% MARIO
\begin{frame}{Mammoth Dataset}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{./images/mammoth3D.png}
        \caption{3D Mammoth Dataset Visualization}
    \end{figure}
\end{frame}

\begin{frame}{Laplacian Eigenmaps Embedding of Mammoth Dataset}
    \begin{figure}
        \centering
        \includegraphics[width=0.7\textwidth]{./images/le_mammoth.png}
        \caption{Laplacian Eigenmaps Embedding of the Mammoth Dataset}
    \end{figure}
    \begin{block}{Observation}
        The effect shown on the picture is called the starfish effect. It reflects perfectly how the Laplacian Eigenmaps preserves connectivity, but it fails to preserve global structure.
    \end{block}
\end{frame}

\begin{frame}{Laplacian Eigenmaps as UMAP initialization}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{./images/UMAP_comparison.png}
        \caption{UMAP Embedding of the Mammoth Dataset with Laplacian Eigenmaps Initialization and Random Initialization}
    \end{figure}
\end{frame}

\end{document}

